{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Super Wav2Lip\n",
        "\n",
        "This Colab project is based on [Wav2Lip-GFPGAN](https://github.com/ajay-sainy/Wav2Lip-GFPGAN), but updates the requirements.txt (to function properly) and updates Colab file for ease of use."
      ],
      "metadata": {
        "id": "JHXiEHrtVVxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installation\n",
        "\n",
        "Run this block to install the necessary dependencies."
      ],
      "metadata": {
        "id": "VBrG_nOFpmWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "git@github.com:JupyterJones/Wav2LipGPU.git"
      ],
      "metadata": {
        "id": "5m5fGPv-_aMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JupyterJones/Wav2LipGPU.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzNmzq6L_w-o",
        "outputId": "106ed7fd-34c1-4bcf-a207-089f4e529ee0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Wav2LipGPU'...\n",
            "remote: Enumerating objects: 162, done.\u001b[K\n",
            "remote: Counting objects:   9% (1/11)\u001b[K\rremote: Counting objects:  18% (2/11)\u001b[K\rremote: Counting objects:  27% (3/11)\u001b[K\rremote: Counting objects:  36% (4/11)\u001b[K\rremote: Counting objects:  45% (5/11)\u001b[K\rremote: Counting objects:  54% (6/11)\u001b[K\rremote: Counting objects:  63% (7/11)\u001b[K\rremote: Counting objects:  72% (8/11)\u001b[K\rremote: Counting objects:  81% (9/11)\u001b[K\rremote: Counting objects:  90% (10/11)\u001b[K\rremote: Counting objects: 100% (11/11)\u001b[K\rremote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 162 (delta 0), reused 9 (delta 0), pack-reused 151\u001b[K\n",
            "Receiving objects: 100% (162/162), 50.17 MiB | 46.04 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "basePath = \"Wav2LipGPU\"\n",
        "%cd {basePath}\n",
        "\n",
        "wav2lipFolderName = 'Wav2Lip-master'\n",
        "gfpganFolderName = 'GFPGAN-master'\n",
        "wav2lipPath = basePath + '/' + wav2lipFolderName\n",
        "gfpganPath = basePath + '/' + gfpganFolderName\n",
        "\n",
        "!wget 'https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth' -O {wav2lipPath}'/face_detection/detection/sfd/s3fd.pth'\n",
        "\n",
        "!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O {wav2lipPath}'/checkpoints/wav2lip_gan.pth'\n",
        "#!wget 'https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW' -O {wav2lipPath}'/checkpoints/wav2lip.pth'\n",
        "\n",
        "!gdown https://drive.google.com/uc?id=1fQtBSYEyuai9MjBOF8j7zZ4oQ9W2N64q --output {wav2lipPath}'/checkpoints/'\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U librosa==0.8.1 # The process will fail without downgrading librosa\n",
        "!mkdir inputs\n",
        "\n",
        "!cd $gfpganFolderName && python setup.py develop\n",
        "!wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P {gfpganFolderName}'/experiments/pretrained_models'\n",
        "\n",
        "%cd {basePath}\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "print(\"Installation complete.\")"
      ],
      "metadata": {
        "id": "YhFe3CJGAIiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ec10b-4232-45ec-acf7-714a53298049"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install basicsr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs2P1Hbmodmk",
        "outputId": "1a1b726e-51e9-42ed-f80f-1e71ddfd4bfd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: basicsr in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from basicsr) (2.4.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from basicsr) (0.18.3)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.10/dist-packages (from basicsr) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from basicsr) (1.22.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from basicsr) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from basicsr) (8.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from basicsr) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from basicsr) (2.27.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from basicsr) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from basicsr) (1.10.1)\n",
            "Requirement already satisfied: tb-nightly in /usr/local/lib/python3.10/dist-packages (from basicsr) (2.14.0a20230603)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from basicsr) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from basicsr) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from basicsr) (4.65.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from basicsr) (0.33.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->basicsr) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->basicsr) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->basicsr) (16.0.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->basicsr) (3.4)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->basicsr) (23.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tb-nightly->basicsr) (0.40.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->basicsr) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly->basicsr) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly->basicsr) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly->basicsr) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->basicsr) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly->basicsr) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly->basicsr) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputPath =\"../../../../content/Wav2LipGPU\""
      ],
      "metadata": {
        "id": "G_91amr7KM7x"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "os.listdir(outputPath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7177b18-8364-4c2b-869d-138062a610fb",
        "id": "18LQNzvEgiZ7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['requirements.txt',\n",
              " 'evaluation',\n",
              " 'wav2lip_train.py',\n",
              " 'GFPGAN-master',\n",
              " 'color_syncnet_train.py',\n",
              " 'hq_wav2lip_train.py',\n",
              " 'temp',\n",
              " 'results',\n",
              " 'README.md',\n",
              " 'filelists',\n",
              " 'Wav2LipGPU',\n",
              " 'preprocess.py',\n",
              " 'audio.py',\n",
              " 'sample_data',\n",
              " 'face_detection',\n",
              " 'hparams.py',\n",
              " 'inference.py',\n",
              " '.git',\n",
              " 'inputs',\n",
              " 'checkpoints',\n",
              " 'models']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ../../../Wav2LipGPU/"
      ],
      "metadata": {
        "id": "HaJKweCw9jqX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../../../Wav2LipGPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GIR9E1t5Pf3",
        "outputId": "f40a1d0f-cfe9-4578-f197-ed7816705c17"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio.py\t\tfilelists\t     inputs\t       results\n",
            "checkpoints\t\tGFPGAN-master\t     models\t       sample_data\n",
            "color_syncnet_train.py\thparams.py\t     preprocess.py     temp\n",
            "evaluation\t\thq_wav2lip_train.py  README.md\t       Wav2LipGPU\n",
            "face_detection\t\tinference.py\t     requirements.txt  wav2lip_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_CoPEOLNM0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IsQ_n_U6NN4H"
      },
      "outputs": [],
      "source": [
        "wav2lipFolderName=\"../../../Wav2LipGPU\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Synchronize Video and Speech"
      ],
      "metadata": {
        "id": "G0BMWoIyX2dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"../../../Wav2LipGPU\")"
      ],
      "metadata": {
        "id": "oojv7GzXCZOl"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60533c43-1adb-4470-f185-b901eac6c192",
        "id": "-pPcDa6xO_A0"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio.py\t\tfilelists\t     inputs\t       results\n",
            "checkpoints\t\tGFPGAN-master\t     models\t       sample_data\n",
            "color_syncnet_train.py\thparams.py\t     preprocess.py     temp\n",
            "evaluation\t\thq_wav2lip_train.py  README.md\t       Wav2LipGPU\n",
            "face_detection\t\tinference.py\t     requirements.txt  wav2lip_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/Wav2LipGPU/Wav2LipGPU/Wav2LipGPU/Wav2Lip-master/checkpoints/wav2lip.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoljdNsjP8-n",
        "outputId": "552de18e-9eb0-43ce-e4f1-36bfafeb45f3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Wav2LipGPU/Wav2LipGPU/Wav2LipGPU/Wav2Lip-master/checkpoints/wav2lip.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZiJRgVLPwNf",
        "outputId": "96464fff-23a8-4c73-b808-200893e736d7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01207-.jpg  input_audio_1.wav\t  input_video_1.mp4\tinput_video-onet.mp4\n",
            "1.mp3\t    input_audio_2.wav\t  input_video_2.mp4\tmisc\n",
            "2.mp3\t    input_audio_3.wav\t  input_video_3.mp4\tvideo01\n",
            "3.mp3\t    input_audio_4.wav\t  input_video_4.mp4\tvideo02\n",
            "4.mp3\t    input_audio_5.wav\t  input_video_5.mp4\tvideo03\n",
            "5.mp3\t    input_audio_6.wav\t  input_video_6.mp4\tvideo04\n",
            "6.mp3\t    input_audio_7.wav\t  input_video_7.mp4\tvideo05\n",
            "7.mp3\t    input_audio_8-14.wav  input_video_8-14.mp4\tvideo06\n",
            "8.mp3\t    input_audio_8.wav\t  input_video_8.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"/content/Wav2LipGPU/Wav2LipGPU/Wav2LipGPU/Wav2Lip-master/checkpoints/wav2lip.pth\"\n",
        "inputVideoPath=\"sample_data/input_video_1.mp4\"\n",
        "inputAudioPath=\"sample_data/1.mp3\"\n",
        "!echo {model}\n",
        "!echo {inputVideoPath}\n",
        "!echo {inputAudioPath}\n",
        "!python inference.py \\\n",
        "--checkpoint_path checkpoints/{model}.pth \\\n",
        "--face {inputVideoPath} \\\n",
        "--audio {inputAudioPath} \\\n",
        "--outfile {lipSyncedOutputPath}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZakW9cjcPj9f",
        "outputId": "26445a64-100b-49cd-92f5-cf24cb3313fe"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Wav2LipGPU/Wav2LipGPU/Wav2LipGPU/Wav2Lip-master/checkpoints/wav2lip.pth\n",
            "sample_data/input_video_1.mp4\n",
            "sample_data/1.mp3\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Wav2LipGPU/inference.py\", line 3, in <module>\n",
            "    import scipy, cv2, os, sys, argparse, audio\n",
            "  File \"/content/Wav2LipGPU/audio.py\", line 102\n",
            "    fmin=hp.fmin, fmax=hp.fmax)\n",
            "IndentationError: unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "EqX_2YtkUjRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1566cc66-de26-42c8-d1e5-f93a10dc436b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/Wav2LipGPU/inference.py\", line 3, in <module>\n",
            "    import scipy, cv2, os, sys, argparse, audio\n",
            "  File \"/content/Wav2LipGPU/audio.py\", line 102\n",
            "    fmin=hp.fmin, fmax=hp.fmax)\n",
            "IndentationError: unexpected indent\n"
          ]
        }
      ],
      "source": [
        "#@markdown Adjust the parameters below then run the code block to synthesize the speech onto your video file:\n",
        "\n",
        "import os\n",
        "#outputPath = '../../../Wav2LipGPU/basePath/outputs/' \n",
        "inputAudio = '1.mp3' #@param{type:\"string\"}\n",
        "inputAudioPath = 'Wav2Lip-master/basePath/outputs/' \n",
        "inputVideo = 'input_video_1.mp4' #@param{type:\"string\"}\n",
        "inputVideoPath = basePath + '/inputs/'+inputVideo\n",
        "lipSyncedOutputPath = basePath + '/outputs/result.mp4'  \n",
        "model = \"wav2lip\" #@param [\"wav2lip\", \"wav2lip_gan\"] {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "!cd $wav2lipFolderName && python inference.py \\\n",
        "--checkpoint_path checkpoints/{model}.pth \\\n",
        "--face {inputVideoPath} \\\n",
        "--audio {inputAudioPath} \\\n",
        "--outfile {lipSyncedOutputPath}\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Video synthesis complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Boost the Resolution of the Synthesized Video\n",
        "\n"
      ],
      "metadata": {
        "id": "oAIIanxYqFcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/wav2lip-HD/outputs/restored_imgs/\n"
      ],
      "metadata": {
        "id": "2jHM5JKWo1EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls "
      ],
      "metadata": {
        "id": "9W4_Q26UpiR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/wav2lip-HD/outputs/restored_imgs/\n"
      ],
      "metadata": {
        "id": "C8O7M43KpoHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/wav2lip-HD/outputs/restored_imgs/\n"
      ],
      "metadata": {
        "id": "-nPO0VoEpmVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0f4ee35-af58-4434-e9c6-ba7d8c7b783f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/wav2lip-HD/outputs/restored_imgs/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from os import path\n",
        "\n",
        "import os\n",
        "\n",
        "inputVideoPath = outputPath+'/result.mp4'\n",
        "unProcessedFramesFolderPath = outputPath+'/frames'\n",
        "\n",
        "if not os.path.exists(unProcessedFramesFolderPath):\n",
        "  os.makedirs(unProcessedFramesFolderPath)\n",
        "\n",
        "vidcap = cv2.VideoCapture(inputVideoPath)\n",
        "numberOfFrames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "print(\"FPS: \", fps, \"Frames: \", numberOfFrames)\n",
        "\n",
        "for frameNumber in tqdm(range(numberOfFrames)):\n",
        "    _,image = vidcap.read()\n",
        "    cv2.imwrite(path.join(unProcessedFramesFolderPath, str(frameNumber).zfill(4)+'.jpg'), image)\n",
        "\n",
        "\n",
        "!cd $gfpganFolderName && \\\n",
        "  python inference_gfpgan.py -i $unProcessedFramesFolderPath -o $outputPath -v 1.3 -s 2 --only_center_face --bg_upsampler None\n",
        "\n",
        "import os\n",
        "restoredFramesPath = outputPath + '/restored_imgs/'\n",
        "processedVideoOutputPath = outputPath\n",
        "\n",
        "dir_list = os.listdir(restoredFramesPath)\n",
        "dir_list.sort()\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#Get FPS of original video for writer\n",
        "inputVideoPath = outputPath+'/result.mp4'\n",
        "vidcap = cv2.VideoCapture(inputVideoPath)\n",
        "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "print(\"The video is \"+str(fps)+\" FPS.\")\n",
        "\n",
        "batch = 0\n",
        "batchSize = 1300\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(0, len(dir_list), batchSize)):\n",
        "  img_array = []\n",
        "  start, end = i, i+batchSize\n",
        "  print(\"processing \", start, end, end=\"\\r\")\n",
        "  for filename in  tqdm(dir_list[start:end]):\n",
        "      filename = restoredFramesPath+filename;\n",
        "      img = cv2.imread(filename)\n",
        "      if img is None:\n",
        "        continue\n",
        "      height, width, layers = img.shape\n",
        "      size = (width,height)\n",
        "      img_array.append(img)\n",
        "  out = cv2.VideoWriter(processedVideoOutputPath+'/output_'+str(batch).zfill(4)+'.mp4',cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
        "  batch = batch + 1\n",
        " \n",
        "  for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "  out.release()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "print(\"Video upscaling complete.\")"
      ],
      "metadata": {
        "id": "XibzGPIVJfvP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8e50c3c-dfe1-4ebb-ae5b-c02cbad1bab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video upscaling complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXPV2_s1pPy7",
        "outputId": "1a824e70-4cc2-448b-d254-f5db37fd564c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "colab.ipynb    inputs\tREADME.md\t  Wav2Lip-master\n",
            "GFPGAN-master  outputs\trequirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Clear Cached Files\n",
        "\n",
        "Run this block once you've downloaded your final video file. This will empty /inputs and /outputs, so you can start again, fresh.\n"
      ],
      "metadata": {
        "id": "ajuOpF9t0psE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8JhzA9t_xrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/wav2lip-HD/\n",
        "\n",
        "#@markdown Choose whether to remove both inputs and outputs, or just one of the two. You may want to preserve inputs if you are only changing one of the two inputs. \n",
        "\n",
        "removeInputs = True #@param {type:\"boolean\"}\n",
        "removeOutputs = True #@param {type:\"boolean\"}\n",
        "\n",
        "if removeInputs == True:\n",
        "  %rm inputs/*\n",
        "if removeOutputs == True:\n",
        "  %rm outputs/frames/*\n",
        "  %rm outputs/restored_imgs/*\n",
        "  %rm outputs/*\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "print(\"Cleared cached files.\")\n"
      ],
      "metadata": {
        "id": "o2trJyiC0mRs",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfaa8830-f224-44e0-d032-f4266eed2a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleared cached files.\n"
          ]
        }
      ]
    }
  ]
}